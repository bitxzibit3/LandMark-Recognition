{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrHMg1kovn6n"
   },
   "source": [
    "## Установка и импорт модулей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mQlRyIIUeZCD"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.pytorch as Ap\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import plotly.express as px\n",
    "import sklearn\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as tf\n",
    "from torchvision.datasets import DatasetFolder \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# if not os.path.exists('./drive'):\n",
    "#     drive.mount('./drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uG61REZqesZs",
    "outputId": "c680e034-17de-491e-f017-bee65f18efbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "#     !mkdir /root/.kaggle\n",
    "#     !cp ./drive/MyDrive/kaggle.json /root/.kaggle/kaggle.json\n",
    "#     !kaggle datasets download -d andreybeyn/qudata-gembed-landmarks-210\n",
    "#     !unzip -q qudata-gembed-landmarks-210.zip\n",
    "\n",
    "!mkdir ./models ./models/models ./models/desc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "ak-wNNv1fMs6",
    "outputId": "8392f904-7b2d-4079-a05a-8214f8995e7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/timur/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "QyxHg2A9gaH5",
    "outputId": "40fae9ba-0523-430b-d62d-a234d888ec09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbitxzibit3\u001B[0m (\u001B[33mml_landmarks\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/timur/LandMark-Recognition/wandb/run-20230422_223740-ik14x1u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml_landmarks/ml_landmarks/runs/ik14x1u5' target=\"_blank\">zesty-gorge-22</a></strong> to <a href='https://wandb.ai/ml_landmarks/ml_landmarks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml_landmarks/ml_landmarks' target=\"_blank\">https://wandb.ai/ml_landmarks/ml_landmarks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml_landmarks/ml_landmarks/runs/ik14x1u5' target=\"_blank\">https://wandb.ai/ml_landmarks/ml_landmarks/runs/ik14x1u5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project = 'ml_landmarks',\n",
    "    entity = 'ml_landmarks',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbL0U5EtcV-6"
   },
   "source": [
    "# Пайплайн примерно такой:\n",
    "\n",
    "\n",
    "*   Обрабатываем данные:\n",
    "\n",
    "    * Считываем данные, перегоняем в тензоры\n",
    "\n",
    "    * Мб делаем нормализацию (пока нет)\n",
    "\n",
    "    * Разбиваем на тренировочную/валидационную\n",
    "\n",
    "*   Пишем сетки: пробуем менять архитектуру, если совсем голяк - меняем предобработку.\n",
    "\n",
    "*   Попробовать сделать ансамбли: бэггинг!, бустинг.\n",
    "\n",
    "*   Оценивать будем `F1`, скорее всего.\n",
    "\n",
    "*   Если все совсем совсем плохо:\n",
    "\n",
    "    * Пробовать более сильные ансамбли. Если тут голяк - пробовать еще:)\n",
    "    \n",
    "    * Будем пробовать аугментацию, потому что картинок реально мало\n",
    "    \n",
    "    * Можно будет попробовать найти похожую сетку (похожую, исходя из поставленной задачи), и попробовать её зафайнтьюнить.\n",
    "    \n",
    "    * Брать другой датасет. Есть сразу проблемы: они в большинстве своем неразмечены (те, которые я находил).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBJY1ezZXc_k"
   },
   "source": [
    "## Чтение данных\n",
    "\n",
    "Тут я попробовал поиграть с вариантами хранения данных. Где то считывал данные, и хранил уже обработанные тензоры, где то считывал данные, и обрабатывал только при необходимости. Также использовал класс, встроенный в `torchvision`. В конце привел сравнение работы всех классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOmvaLsLVEAy"
   },
   "source": [
    "В датасете есть черно-белые фотографии, и фотографии, в которых 4 канала, а не 3 по стандарту. Таких картинок немного, поэтому удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vD3XN3wd_Wch",
    "outputId": "5b45bf37-fcb2-442c-b398-c37c07545573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files: 10515\n",
      "Deleted files: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_path = './data/landmarks/'\n",
    "\n",
    "# Get all filepaths\n",
    "all_files = set()\n",
    "labels = []\n",
    "for path, dirs, files in os.walk(init_path):\n",
    "    if dirs == []:\n",
    "        for file_ in files:\n",
    "            filepath = '/'.join([path, file_])\n",
    "            all_files.add(filepath)\n",
    "    else:\n",
    "        labels.extend(dirs)\n",
    "\n",
    "# Filtering\n",
    "supported_types = ('RGB')\n",
    "\n",
    "incorrect_files = set()\n",
    "for filename in all_files:\n",
    "    img = PIL.Image.open(filename)\n",
    "    if img.mode not in supported_types:\n",
    "        incorrect_files.add(filename)\n",
    "    del img\n",
    "\n",
    "all_files = list(all_files - incorrect_files)\n",
    "print('\\n'.join([f'All files: {len(all_files)}',\n",
    "                 f'Deleted files: {len(incorrect_files)}']))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnSm8eSra48Z"
   },
   "source": [
    "### `FastDataset`\n",
    "Тут я сделал класс датасета с быстрым доступом.\n",
    "\n",
    "Мы сразу по названию файла обрезаем её, делаем из картинки тензор, запоминаем его, потом получаем к нему доступ просто по индексу, не делая никакой предобработки.\n",
    "\n",
    "+: Быстро бегаем\n",
    "\n",
    "-: Долгая инициализация\n",
    "\n",
    "-: В теории, может сожрать всю память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCHpJcJcGacG"
   },
   "outputs": [],
   "source": [
    "class FastDataset(Dataset):\n",
    "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
    "                 transform = None,\n",
    "                 image_shape = (200, 200)):\n",
    "        \n",
    "        '''\n",
    "        mode - train/valid/test\n",
    "        labels - list with all possible namelabels\n",
    "        transform - proccessing of file\n",
    "        image_shape - shape of result tensor\n",
    "        '''\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.image_shape = image_shape\n",
    "        self.transform = transform if transform \\\n",
    "        else tf.Compose([tf.Resize(image_shape), tf.PILToTensor()])\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.device = device\n",
    "\n",
    "        self.check_mode = self.mode in ('train', 'valid')\n",
    "\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "        # Saving tensors from PIL.Image\n",
    "        for path in files:\n",
    "            label = path.split('/')[-2]\n",
    "            tensor = self.get_sample(path)\n",
    "            self.x.append(tensor / 255)\n",
    "            self.y.append(label)\n",
    "\n",
    "        self._len = len(self.x)\n",
    "\n",
    "    def get_sample(self, filepath):\n",
    "        with PIL.Image.open(filepath) as image:\n",
    "            image = PIL.Image.open(filepath)\n",
    "            tensor = self.transform(image)\n",
    "        return tensor\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns Tensor, str (optional)\n",
    "        '''\n",
    "        if self.check_mode:\n",
    "            y = self.le.transform([self.y[idx]])\n",
    "            return self.x[idx], y[0]\n",
    "        else:\n",
    "            return self.x[idx]\n",
    "\n",
    "    def decode(self, num_label):\n",
    "        return self.le.inverse_transform([num_label])[0]\n",
    "\n",
    "    def train_valid_split(self, train_size = 0.9):\n",
    "        '''\n",
    "        Unfirom split of files.\n",
    "\n",
    "        Returns two datasets: train_dataset and valid_dataset\n",
    "        '''\n",
    "        def handle_one_class(label):\n",
    "            file_list = get_class_samples(label)\n",
    "            train_set, valid_set = train_test_split(tuple(file_list),\n",
    "                                                    train_size = train_size)\n",
    "            return train_set, valid_set\n",
    "\n",
    "        def get_class_samples(label):\n",
    "            return set([filename\n",
    "            for filename in self.files if label in filename.split('/')])\n",
    "\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        labels = self.le.classes_\n",
    "        \n",
    "        for label in labels:\n",
    "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
    "            train_list.extend(cur_train_list)\n",
    "            valid_list.extend(cur_valid_list)\n",
    "\n",
    "        train_ds = FastDataset(mode = 'train',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = train_list)\n",
    "\n",
    "        valid_ds = FastDataset(mode = 'valid',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = valid_list)\n",
    "        return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dyv0_7gbQyx"
   },
   "source": [
    "### `CustomDataset`\n",
    "\n",
    "Это тоже класс датасета, но с медленным доступом. Здесь мы запоминаем все пути до картинок, потом при получении по индексу делаем предобработку, типа ресайз и перегоняем в тензор.\n",
    "\n",
    "+: Жрет немного памяти (ну во всяком случае меньше, чем `FastDataset`)\n",
    "\n",
    "+: Быстрая иницилазция\n",
    "\n",
    "-: Долго бегает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeU6HdblouLU"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
    "                 transform = None,\n",
    "                 image_shape = (200, 200)):\n",
    "        \n",
    "        '''\n",
    "        mode - train/valid/test\n",
    "        files - list/set with filepaths\n",
    "        labels - list with all possible namelabels\n",
    "        transform - proccessing of file\n",
    "        image_shape - shape of result tensor\n",
    "        '''\n",
    "\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.image_shape = image_shape\n",
    "        self.files = files\n",
    "        \n",
    "        self.check_mode = self.mode in ('train', 'test')\n",
    "        \n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def default_transform(self, img):\n",
    "        '''\n",
    "        Make image resizing, and converting to tensor\n",
    "        '''\n",
    "        transform = tf.Compose([\n",
    "            tf.Resize(self.image_shape),\n",
    "            tf.PILToTensor()\n",
    "        ])\n",
    "        return transform(img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        with PIL.Image.open(path) as img:\n",
    "            if self.transform:\n",
    "                tensor = self.transform(img)\n",
    "            else:\n",
    "                tensor = self.default_transform(img)\n",
    "\n",
    "        if self.check_mode:\n",
    "            label = self.get_label(idx)\n",
    "            return tensor, self.le.transform([label])[0]\n",
    "        else:\n",
    "            return tensor\n",
    "\n",
    "    def get_label(self, idx):\n",
    "        assert self.check_mode, \\\n",
    "        'It is not possible to get label'\n",
    "        path = self.files[idx]\n",
    "        return path.split('/')[2]\n",
    "\n",
    "    def decode(self, num_label):\n",
    "        return self.le.inverse_transform([num_label])[0]\n",
    "\n",
    "    def train_valid_split(self, train_size = 0.9):\n",
    "        '''\n",
    "        Unfirom split of files.\n",
    "\n",
    "        Returns two datasets: train_dataset and valid_dataset (augmentations = [None])\n",
    "        '''\n",
    "        def handle_one_class(label):\n",
    "            file_list = get_class_samples(label)\n",
    "            train_set, valid_set = train_test_split(tuple(file_list),\n",
    "                                                    train_size = train_size)\n",
    "            return train_set, valid_set\n",
    "\n",
    "        def get_class_samples(label):\n",
    "            return set([filename\n",
    "            for filename in self.files if label in filename.split('/')])\n",
    "\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        labels = self.le.classes_\n",
    "        \n",
    "        for label in labels:\n",
    "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
    "            train_list.extend(cur_train_list)\n",
    "            valid_list.extend(cur_valid_list)\n",
    "\n",
    "        train_ds = CustomDataset(mode = 'train',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = train_list)\n",
    "        \n",
    "        valid_ds = CustomDataset(mode = 'valid',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = valid_list)\n",
    "        return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q547p3nH9Llz"
   },
   "source": [
    "### `torchvision.datasets.DatasetFolder`\n",
    "Тоже прогоним через сравнение просто для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jBfmsYLHlJA"
   },
   "outputs": [],
   "source": [
    "def make_DatasetFolder(path = init_path, transform = None,\n",
    "                       extensions = [], image_shape = (200, 200), mode = None,\n",
    "                       **kwargs):\n",
    "    def loader(path):\n",
    "        return PIL.Image.open(path)\n",
    "        \n",
    "    if not transform:\n",
    "        transform = tf.Compose([\n",
    "            tf.Resize(image_shape),\n",
    "            tf.PILToTensor()\n",
    "        ])\n",
    "\n",
    "    extensions = ['jpg', 'jpeg', 'png', 'webp']\n",
    "    return DatasetFolder(path, loader = loader,\n",
    "                         extensions = extensions, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhrc_O1H6wGT"
   },
   "source": [
    "### `AugmentedFastDataset`\n",
    "\n",
    "Версия `FastDataset`, дополненная аугментациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8r2y-MkNdAyv"
   },
   "outputs": [],
   "source": [
    "class AugmentedFastDataset(Dataset):\n",
    "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
    "                 transform = None,\n",
    "                 image_shape = (200, 200)):\n",
    "        '''\n",
    "        mode - train/valid/test\n",
    "        labels - list with all possible namelabels\n",
    "        transform - proccessing of file\n",
    "        image_shape - shape of result tensor\n",
    "        '''\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.image_shape = image_shape\n",
    "        self.transform = transform if transform \\\n",
    "        else tf.Compose([tf.Resize(image_shape), tf.PILToTensor()])\n",
    "        \n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        self.check_mode = self.mode in ('train', 'valid')\n",
    "        self._len = len(files)\n",
    "\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "\n",
    "        self.augmentations = (\n",
    "            None, \n",
    "            tf.ColorJitter(brightness = 0.3,\n",
    "                           contrast = 0.3,\n",
    "                           saturation = 0.3),\n",
    "            tf.RandomPosterize(bits = 2, p = 1),\n",
    "            tf.RandomAdjustSharpness(sharpness_factor = 2,\n",
    "                                     p = 1),\n",
    "            tf.RandomEqualize(p = 1),\n",
    "            tf.RandomRotation(degrees = (-20, 20)),\n",
    "            tf.RandomHorizontalFlip(p = 1)\n",
    "        )\n",
    "\n",
    "        self.augmentations_amount = len(self.augmentations)\n",
    "\n",
    "        # Saving tensors from PIL.Image\n",
    "        for path in files:\n",
    "            label = path.split('/')[-2]\n",
    "            tensor = self.get_sample(path)\n",
    "            augmentations = self.get_augmented_samples(tensor)\n",
    "            self.x.extend(augmentations)\n",
    "            self.y.extend([label] * self.augmentations_amount)\n",
    "\n",
    "    def get_sample(self, filepath):\n",
    "        with PIL.Image.open(filepath) as image:\n",
    "            image = PIL.Image.open(filepath)\n",
    "            tensor = self.transform(image)\n",
    "        return tensor\n",
    "                    \n",
    "    def get_augmented_samples(self, tensor):\n",
    "        answer = [tensor / 255]\n",
    "        answer.extend(\n",
    "            [augmentation(tensor) / 255 \n",
    "            for augmentation in self.augmentations if augmentation]\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len * self.augmentations_amount\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns Tensor, str (optional)\n",
    "        '''\n",
    "        if self.check_mode:\n",
    "            y = self.le.transform([self.y[idx]])\n",
    "            return self.x[idx], y[0]\n",
    "        else:\n",
    "            return self.x[idx]\n",
    "\n",
    "    def decode(self, num_label):\n",
    "        return self.le.inverse_transform([num_label])[0]\n",
    "\n",
    "    def train_valid_split(self, train_size = 0.9):\n",
    "        '''\n",
    "        Unfirom split of files.\n",
    "\n",
    "        Returns two datasets: train_dataset and valid_dataset (augmentations = [None])\n",
    "        '''\n",
    "        def handle_one_class(label):\n",
    "            file_list = get_class_samples(label)\n",
    "            train_set, valid_set = train_test_split(tuple(file_list),\n",
    "                                                    train_size = train_size)\n",
    "            return train_set, valid_set\n",
    "\n",
    "        def get_class_samples(label):\n",
    "            return set([filename\n",
    "            for filename in self.files if label in filename[0].split('/')])\n",
    "\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        labels = self.le.classes_\n",
    "        \n",
    "        for label in labels:\n",
    "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
    "            train_list.extend(cur_train_list)\n",
    "            valid_list.extend(cur_valid_list)\n",
    "\n",
    "        train_ds = AugmentedFastDataset(mode = 'train',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = train_list,\n",
    "                                      augmentations = [None])\n",
    "        train_ds.augmentations = self.augmentations\n",
    "\n",
    "        valid_ds = AugmentedFastDataset(mode = 'valid',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = valid_list,\n",
    "                                      augmentations = [None])\n",
    "        valid_ds.augmentations = self.augmentations\n",
    "        return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62u_hQUT7Dvb"
   },
   "source": [
    "### `AugemntedCustomDataset`\n",
    "\n",
    "Версия `CustomDataset`, дополненная аугментациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nDBuDhHfcLM"
   },
   "outputs": [],
   "source": [
    "class AugmentedCustomDataset(Dataset):\n",
    "    def __init__(self, mode = 'train', files = all_files, labels = labels,\n",
    "                 transform = None,\n",
    "                 image_shape = (200, 200), augmentations = None):\n",
    "        '''\n",
    "        mode - train/valid/test\n",
    "        files - list/set with filepaths\n",
    "        labels - list with all possible namelabels\n",
    "        transform - proccessing of file\n",
    "        image_shape - shape of result tensor\n",
    "        '''\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "        self.check_mode = self.mode in ('train', 'valid')\n",
    "        \n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "\n",
    "        # Initialize augmentation options\n",
    "        if augmentations:\n",
    "            self.augmentations = augmentations\n",
    "        else:\n",
    "            self.augmentations = [\n",
    "                None, \n",
    "                tf.ColorJitter(brightness = 0.3,\n",
    "                            contrast = 0.3,\n",
    "                            saturation = 0.3),\n",
    "                tf.RandomPosterize(bits = 2, p = 1),\n",
    "                tf.RandomAdjustSharpness(sharpness_factor = 2,\n",
    "                                        p = 1),\n",
    "                tf.RandomEqualize(p = 1),\n",
    "                tf.RandomRotation(degrees = (-20, 20)),\n",
    "                tf.RandomHorizontalFlip(p = 1)\n",
    "            ]\n",
    "        self.augmentations_amount = len(self.augmentations)\n",
    "        if self.augmentations == [None]:\n",
    "            self.files = files\n",
    "        \n",
    "        else:\n",
    "            self.files = []\n",
    "            for filename in files:\n",
    "                augmented_filenames = [(filename, i) \n",
    "                                    for i in range(self.augmentations_amount)]\n",
    "                self.files.extend(augmented_filenames)\n",
    "\n",
    "        self._len = len(self.files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def default_transform(self, img):\n",
    "        '''\n",
    "        Make image resizing, and converting to tensor\n",
    "        '''\n",
    "        transform = tf.Compose([\n",
    "            tf.Resize(self.image_shape),\n",
    "            tf.PILToTensor()\n",
    "        ])\n",
    "        return transform(img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find path to file depending on idx\n",
    "        filename, augment_idx = self.files[idx]\n",
    "        augment = self.augmentations[augment_idx]\n",
    "        with PIL.Image.open(filename) as img:\n",
    "            if self.transform:\n",
    "                tensor = self.transform(img)\n",
    "            else:\n",
    "                tensor = self.default_transform(img)\n",
    "            \n",
    "            if augment:\n",
    "                tensor = augment(tensor)\n",
    "\n",
    "            tensor = tensor / 255\n",
    "\n",
    "        if self.check_mode:\n",
    "            label = self.get_label(filename)\n",
    "            return tensor, self.encode(label)\n",
    "        else:\n",
    "            return tensor\n",
    "\n",
    "    def get_label(self, path):\n",
    "        assert self.check_mode, \\\n",
    "        'It is not possible to get label'\n",
    "        return path.split('/')[-2]\n",
    "\n",
    "    def encode(self, str_label):\n",
    "        return self.le.transform([str_label])[0]\n",
    "\n",
    "    def decode(self, num_label):\n",
    "        return self.le.inverse_transform([num_label])[0]\n",
    "\n",
    "    def get_augmented_samples(self, idx):\n",
    "        begin_idx = idx * self.augmentations_amount\n",
    "        return [self[begin_idx + i][0] for i in range(self.augmentations_amount)]\n",
    "    \n",
    "    def draw_augmented_samples(self, idx):\n",
    "        samples = self.get_augmented_samples(idx)\n",
    "        plt.figure(figsize = (20, 20))\n",
    "        for i, sample in enumerate(samples):\n",
    "            plt.subplot(1, len(samples), i + 1)\n",
    "            plt.imshow(sample.permute(1, 2, 0))\n",
    "    \n",
    "    def analyze_splitting(self):\n",
    "        for_plot = {}\n",
    "        for filename in self.files:\n",
    "            label = self.get_label(filename)\n",
    "            if label in for_plot:\n",
    "                for_plot[label] += 1\n",
    "            else:\n",
    "                for_plot[label] = 1\n",
    "        for_plot = pd.DataFrame.from_dict(for_plot, orient = 'index',\n",
    "                                          columns = ['Amount'])\n",
    "        return for_plot\n",
    "\n",
    "    def train_valid_split(self, train_size = 0.9):\n",
    "        '''\n",
    "        Unfirom split of files.\n",
    "\n",
    "        Returns two datasets: train_dataset and valid_dataset (augmentations = [None])\n",
    "        '''\n",
    "        def handle_one_class(label):\n",
    "            file_list = get_class_samples(label)\n",
    "            train_set, valid_set = train_test_split(tuple(file_list),\n",
    "                                                    train_size = train_size)\n",
    "            return train_set, valid_set\n",
    "\n",
    "        def get_class_samples(label):\n",
    "            return set([filename\n",
    "            for filename in self.files if label in filename[0].split('/')])\n",
    "\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        labels = self.le.classes_\n",
    "        \n",
    "        for label in labels:\n",
    "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
    "            train_list.extend(cur_train_list)\n",
    "            valid_list.extend(cur_valid_list)\n",
    "\n",
    "        train_ds = AugmentedCustomDataset(mode = 'train',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = train_list,\n",
    "                                      augmentations = [None])\n",
    "        train_ds.augmentations = self.augmentations\n",
    "\n",
    "        valid_ds = AugmentedCustomDataset(mode = 'valid',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = valid_list,\n",
    "                                      augmentations = [None])\n",
    "        valid_ds.augmentations = self.augmentations\n",
    "        return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBBdnHiefeDU"
   },
   "source": [
    "### `AdvancedCustomDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK7mSjJ2ftmL"
   },
   "source": [
    "В общем то, стало понятно, что обучение даже при использовании `AugmentedCustomDataset` не является эффективным, так как переобучение появляется уже на ранних этапах. Давайте доработаем `AugmentedCustomDataset` таким образом: теперь мы будем применять не фиксированный список возможных трансформаций, а будем дополнять уже существующее множество фотографий до определенного порога, и будем делать это для каждого класса. В результате работы планируется получить набор, который будет содержать одно и то же количество фотографий для каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P6tLb1YHfrRD"
   },
   "outputs": [],
   "source": [
    "class AdvancedCustomDataset(Dataset):\n",
    "    def __init__(self, augmentate, files = all_files, labels = labels, \n",
    "                 ex_amount = 1000, mode = 'train', transform = None,\n",
    "                 image_shape = (200, 200), augmentations = None):\n",
    "        '''\n",
    "        ex_amount - number of photo per class\n",
    "        mode - train/valid/test\n",
    "        files - list/set with filepaths\n",
    "        labels - list with all possible namelabels\n",
    "        transform - proccessing of file\n",
    "        image_shape - shape of result tensor\n",
    "        '''\n",
    "        self.mode = mode\n",
    "        self.transform = transform \\\n",
    "        if transform \\\n",
    "        else A.augmentations.geometric.resize.Resize(*image_shape)\n",
    "        self.image_shape = image_shape\n",
    "        self.ex_amount = ex_amount\n",
    "        self.check_mode = self.mode in ('train', 'valid')\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "\n",
    "        # Initialize augmentation options\n",
    "        if augmentations:\n",
    "            self.augmentations = augmentations\n",
    "        else:\n",
    "            self.augmentations = (\n",
    "                A.ColorJitter(brightness = 0.3,\n",
    "                              contrast = 0.3,\n",
    "                              saturation = 0.3),\n",
    "                A.Posterize(num_bits = 2, p = 1),\n",
    "                A.Sharpen(alpha = (0.9, 1.0)),\n",
    "                A.Equalize(p = 1),\n",
    "                A.Rotate(limit = (-20, 20), p = 1),\n",
    "                A.HorizontalFlip(p = 1)\n",
    "        )\n",
    "        self.augmentations_amount = len(self.augmentations)\n",
    "        self.files = files\n",
    "        if augmentate:\n",
    "            self.files = self.augmentate()\n",
    "\n",
    "        self._len = len(self.files)\n",
    "\n",
    "    def augmentate(self):\n",
    "        labels = self.le.classes_\n",
    "        new_files = []\n",
    "        for label in labels:\n",
    "            new_files_for_label = self.augmentate_one_class(label)\n",
    "            new_files.extend(new_files_for_label)\n",
    "        return new_files\n",
    "\n",
    "    def augmentate_one_class(self, label):\n",
    "        ex_amount = self.ex_amount\n",
    "        files = self.get_class_samples(label)\n",
    "        new_files = []\n",
    "        while len(new_files) < ex_amount:\n",
    "            filename = np.random.choice(files, size = 1)[0]\n",
    "            augmentations_amount = np.random.randint(low = 0,\n",
    "                                                     high = self.augmentations_amount)\n",
    "            if augmentations_amount:\n",
    "                augmentations = np.random.choice(a = self.augmentations,\n",
    "                                                 size = augmentations_amount,\n",
    "                                                 replace = False)\n",
    "                augmentations = A.Compose(augmentations)\n",
    "                new_files.append((filename, augmentations))\n",
    "            else:\n",
    "                new_files.append((filename, None))\n",
    "        return new_files\n",
    "\n",
    "    def get_class_samples(self, label):\n",
    "        return [filename\n",
    "        for filename in self.files if label in filename.split('/')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find path to file depending on idx\n",
    "        filename, augmentations = self.files[idx]\n",
    "        img = cv2.imread(filename)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        tensor = self.transform(image = img)['image']\n",
    "        if augmentations:\n",
    "            tensor = augmentations(image = tensor)['image']\n",
    "\n",
    "        tensor = tensor / 255\n",
    "        tensor = Ap.ToTensorV2()(image = tensor)['image'].float()\n",
    "\n",
    "        if self.check_mode:\n",
    "            label = self.get_label(filename)\n",
    "            return tensor, self.encode(label)\n",
    "        else:\n",
    "            return tensor\n",
    "\n",
    "    def get_augmented_samples(self, idx):\n",
    "        '''\n",
    "        Method to get all augmentations with the same image\n",
    "        idx - index in self.files\n",
    "        '''\n",
    "        filename = self.files[idx][0]\n",
    "        answer = [item for item in self.files\n",
    "                  if filename == item[0]]\n",
    "        return answer\n",
    "\n",
    "    def draw_augmented_samples(self, idx):\n",
    "        files = self.get_augmented_samples(idx)\n",
    "        columns = 5\n",
    "        number = len(files)\n",
    "        if number % columns:\n",
    "            lines = int(number / columns) + 1\n",
    "        else:\n",
    "            lines = int(number / columns)\n",
    "        print(f'{number}: {lines}:{columns}')\n",
    "        plt.figure(figsize = (20, 20))\n",
    "        for idx, item in enumerate(files):\n",
    "            filename, augmentation = item\n",
    "            img = cv2.imread(filename)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if augmentation:\n",
    "                img = augmentation(image = img)['image']\n",
    "            plt.subplot(lines, columns, idx + 1)\n",
    "            plt.imshow(img)\n",
    "\n",
    "    def get_label(self, path):\n",
    "        assert self.check_mode, \\\n",
    "        'It is not possible to get label'\n",
    "        return path.split('/')[-2]\n",
    "\n",
    "    def encode(self, str_label):\n",
    "        return self.le.transform([str_label])[0]\n",
    "\n",
    "    def decode(self, num_label):\n",
    "        return self.le.inverse_transform([num_label])[0]\n",
    "\n",
    "    def train_valid_split(self, train_size = 0.9):\n",
    "        '''\n",
    "        Unfirom split of files.\n",
    "\n",
    "        Returns two datasets: train_dataset and valid_dataset\n",
    "        '''\n",
    "        def handle_one_class(label):\n",
    "            file_list = get_class_samples(label)\n",
    "            train_set, valid_set = train_test_split(tuple(file_list),\n",
    "                                                    train_size = train_size)\n",
    "            return train_set, valid_set\n",
    "\n",
    "        def get_class_samples(label):\n",
    "            return set([filename\n",
    "            for filename in self.files if label in filename[0].split('/')])\n",
    "\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        labels = self.le.classes_\n",
    "        \n",
    "        for label in labels:\n",
    "            cur_train_list, cur_valid_list = handle_one_class(label)\n",
    "            train_list.extend(cur_train_list)\n",
    "            valid_list.extend(cur_valid_list)\n",
    "\n",
    "        train_ds = AdvancedCustomDataset(augmentate = False, mode = 'train',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = train_list)\n",
    "        train_ds.augmentations = self.augmentations\n",
    "\n",
    "        valid_ds = AdvancedCustomDataset(augmentate = False, mode = 'valid',\n",
    "                                      labels = labels,\n",
    "                                      image_shape = self.image_shape,\n",
    "                                      files = valid_list)\n",
    "        valid_ds.augmentations = self.augmentations\n",
    "        return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tPpz2ClN7wH"
   },
   "source": [
    "### Сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zcvuJ9EF7Tv"
   },
   "outputs": [],
   "source": [
    "def memory_counter(ex, all = False):\n",
    "    '''\n",
    "    Memory counter for existing class instance\n",
    "    all - count all variables and methods in ex, else exclude __methods__\n",
    "    '''\n",
    "    mem = 0\n",
    "    if all:\n",
    "        for key, val in ex.__dict__.items():\n",
    "            mem += sys.getsizeof(val)\n",
    "        return mem\n",
    "    else:\n",
    "        for key, val in ex.__dict__.items():\n",
    "            if key.startswith('_'):\n",
    "                continue\n",
    "            else:\n",
    "                mem += sys.getsizeof(val)\n",
    "        return mem\n",
    "\n",
    "def dataset_metric(cls, print_info = True, **kwargs):\n",
    "    '''\n",
    "    Comparing of classes with datasets: init, traverse, memory\n",
    "    '''\n",
    "    print(f'Class name: {cls.__name__}')\n",
    "    begin = time.time()\n",
    "    ex = cls(**kwargs)\n",
    "    to_init = time.time() - begin\n",
    "    print('Time to init: {:.5f} s'.format(to_init))\n",
    "    begin = time.time()\n",
    "    for _ in ex:\n",
    "        pass\n",
    "    to_traverse = time.time() - begin\n",
    "    print('Time to traverse: {:.5f} s'.format(to_traverse))\n",
    "    memory = memory_counter(ex)\n",
    "    info = '\\n'.join(['Memory: {} bytes = {:.3f} MB',\n",
    "                    'Total elements: {} elements',\n",
    "                    'Mean iteration time: {:.4f} s',\n",
    "                    'Mean memory usage per element: {:.4f} bytes',\n",
    "                     '']).format(memory, memory / 10 ** 6, \n",
    "                                 len(ex), \n",
    "                                 to_traverse / len(ex),\n",
    "                                 memory / len(ex))\n",
    "    if print_info:\n",
    "        print(info)\n",
    "    \n",
    "    d = (cls.__name__, to_init, to_traverse, memory)\n",
    "    del ex\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuD_S8CGNiyn",
    "outputId": "0f6d8038-a724-4f2a-922b-ed29081fb1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: AdvancedCustomDataset\n",
      "Time to init: 22.53049 s\n",
      "Time to traverse: 1930.73612 s\n",
      "Memory: 1764466 bytes = 1.764 MB\n",
      "Total elements: 210000 elements\n",
      "Mean iteration time: 0.0092 s\n",
      "Mean memory usage per element: 8.4022 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "advds_metric = dataset_metric(AdvancedCustomDataset, augmentate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXEd3l6hPayL",
    "outputId": "65b99c19-91ae-492c-92b9-399667a8434d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: CustomDataset\n",
      "Time to init: 0.00100 s\n",
      "Time to traverse: 126.87075 s\n",
      "Memory: 84378 bytes = 0.084 MB\n",
      "Total elements: 10515 elements\n",
      "Mean iteration time: 0.0121 s\n",
      "Mean memory usage per element: 8.0245 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cds_metric = dataset_metric(CustomDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBiLKY-A0YX7",
    "outputId": "76de6c21-93ed-4e00-a0a7-5e452b8a2bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: FastDataset\n",
      "Time to init: 131.19805 s\n",
      "Time to traverse: 3.40911 s\n",
      "Memory: 170610 bytes = 0.171 MB\n",
      "Total elements: 10515 elements\n",
      "Mean iteration time: 0.0003 s\n",
      "Mean memory usage per element: 16.2254 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fds_metric = dataset_metric(FastDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOkRJxsv0dp7",
    "outputId": "f4abb65d-8852-42c1-d0a9-97dd82c7cb3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: make_DatasetFolder\n",
      "Time to init: 0.09396147727966309\n",
      "Time to traverse: 127.35288572311401\n",
      "Memory: 2409920 bytes = 2.410 MB\n",
      "Total elements: 10534 elements\n",
      "Mean iteration time: 0.012\n",
      "Mean memory per element: 228.775 bytes\n"
     ]
    }
   ],
   "source": [
    "dfds_metric = dataset_metric(make_DatasetFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xNRbcfM7gy_",
    "outputId": "d05e6566-9124-4ea7-fdbd-487b2ce75fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: AugmentedCustomDataset\n",
      "Time to init: 0.020332813262939453\n",
      "Time to traverse: 1068.542881011963\n",
      "Memory: 6457296 bytes = 6.457 MB\n",
      "Total elements: 73605 elements\n",
      "Mean iteration time: 0.015\n",
      "Mean memory per element: 87.729 bytes\n"
     ]
    }
   ],
   "source": [
    "acds_metric = dataset_metric(AugmentedCustomDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mns_79Kqatr6"
   },
   "source": [
    "### Выводы\n",
    "Тут надо подумать и выбрать, пока буду юзать Fast.\n",
    "\n",
    "Стоит еще заметить вот что: при работе с `torchvision.datasets.DatasetFolder` у нас классификация происходит иначе, нежели в остальных классах. Соответственно, если обучить сетку, а потом поменять тип используемого датасета, то будет плохо. Поэтому на этом мы прощаемся этой штукой. Не очень грустно, потому что в сравнении с другими вариантами она не сказать что превосходит по времени/памяти.\n",
    "\n",
    "В перспективе, конечно, правильнее будет использовать `CustomDataset`, потому что при расширении датасета для `FastDataset` может банально не хватить памяти. Пока же, мы будем использовать `FastDataset`, и если что, добавлю возможность переключения на `CustomDataset`.\n",
    "\n",
    "Но как выяснилось, данных слишком мало, поэтому их пришлось аугментировать. Создал два новых класса: `AugmentedCustomDataset` - аналог `CustomDataset` с возможностью аугментации, и `AugmentedFastDataset` - аналог `FastDataset` с возможностью аугментации.\n",
    "\n",
    "Далее, будем использовать `AugmentedCustomDataset`, потому что быстрый аналог банально перестал влезать в память. Но придется потерять во времени..("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrEARZntOa-v"
   },
   "source": [
    "Также, во время прогонки `AdvancedCustomDataset` выяснилось, что методы аугментации из `torchvision.transforms` работают несколько дольше, нежели аналоги из `albumentations`. Поэтому в `AdvancedCustomDataset` будут использоваться методы модуля `albumentations`.\n",
    "\n",
    "А еще, оказалось, что считывание через `cv2.imread` работает быстрее,чем `PIL.Image.open`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS1ysaaPZdkh"
   },
   "source": [
    "## Разбиение данных\n",
    "\n",
    "Это я выделил в отдельный раздельчик, причина, как по мне, существенная: картинок очень мало (по 50 на класс), классов очень много (210) и хотелось бы проконтроллировать, чтоб в тренировочной выборке был баланс классов. Я думаю, что в тренировочную выборку мы закинем 90% (для начала, дальше видно будет). Возможно, придется обучать на всей выборке, а потом валидиться на каком то подмножестве тренировочной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sl2RxSztsh3n"
   },
   "outputs": [],
   "source": [
    "def make_loaders(ds_cls, train_size, train_bs, valid_bs, ds_params):\n",
    "    '''\n",
    "    ds_cls - class of using dataset\n",
    "    Return two DataLoaders: train and valid\n",
    "    '''\n",
    "\n",
    "    ds = ds_cls(**ds_params)\n",
    "    train_ds, valid_ds = ds.train_valid_split(train_size = train_size)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size = train_bs,\n",
    "                          shuffle = True, num_workers = 1)\n",
    "\n",
    "    valid_dl = DataLoader(valid_ds, batch_size = valid_bs,\n",
    "                          shuffle = False, num_workers = 1)\n",
    "    \n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wGXQ3ejatRj"
   },
   "source": [
    "## Цикл обучения с валидацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yDnzHsDOwAT9"
   },
   "outputs": [],
   "source": [
    "def train_valid(model, train_dl, valid_dl,\n",
    "                opt_cls, opt_params, loss_fn, \n",
    "                metric_fn, max_epochs:int,\n",
    "                device,\n",
    "                scheduler_cls = None, scheduler_params = None):\n",
    "    '''\n",
    "    Train and validation cycle.\n",
    "    \n",
    "    train_dl - DataLoader with train data\n",
    "    valid_dl - DataLoader with valid data\n",
    "    opt - optimizer\n",
    "    loss_fn - loss function\n",
    "    metric_fn - metric function to evaluate model\n",
    "    max_epochs - epochs to training and validation\n",
    "    scheduler_cls - class of scheduler\n",
    "    '''\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_metric = []\n",
    "    valid_metric = []\n",
    "    \n",
    "    def print_loss_metric_info(train_loss = train_losses, \n",
    "                               valid_loss = valid_losses,\n",
    "                               train_metric = train_metric, \n",
    "                               valid_metric = valid_metric):\n",
    "        '''\n",
    "        Logger function\n",
    "        '''\n",
    "        template = '\\n'.join(['',\n",
    "                              'Losses on train: {}',\n",
    "                              'Losses on valid: {}',\n",
    "                              'Metric on train: {}',\n",
    "                              'Metric on valid: {}'])\n",
    "        print(template.format(train_loss,\n",
    "                               valid_loss,\n",
    "                               train_metric,\n",
    "                               valid_metric))\n",
    "\n",
    "    # Optimizer initialization\n",
    "    opt = opt_cls(params = model.parameters(), \n",
    "                  **opt_params)\n",
    "    \n",
    "    # Scheduler initialization\n",
    "    if scheduler_cls:\n",
    "        scheduler = scheduler_cls(optimizer = opt,\n",
    "                                  **scheduler_params)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    model = model.to(device)\n",
    "    train_time = 0\n",
    "    valid_time = 0\n",
    "    for epoch in tqdm(range(max_epochs), desc = 'Epoch'):\n",
    "    # Training cycle\n",
    "        model.train()\n",
    "        train_losses_epoch = []\n",
    "        train_metric_epoch = []\n",
    "        print_loss_metric_info()\n",
    "        begin_time = time.time()\n",
    "        for x, y in tqdm(train_dl):\n",
    "            opt.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            y_pred = torch.argmax(output, dim = -1)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            metric_value = metric_fn(y.to('cpu'), y_pred.to('cpu'), average = 'macro')\n",
    "            train_metric_epoch.append(metric_value)\n",
    "            train_losses_epoch.append(loss.item())\n",
    "        train_time += (time.time() - begin_time)\n",
    "        train_losses.append(np.mean(train_losses_epoch))\n",
    "        train_metric.append(np.mean(train_metric_epoch))\n",
    "\n",
    "    # Valid cycle\n",
    "        model.eval()\n",
    "        valid_losses_epoch = []\n",
    "        valid_metric_epoch = []\n",
    "        print_loss_metric_info()\n",
    "        begin_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(valid_dl):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = model(x)\n",
    "                y_pred = torch.argmax(output, dim = -1)\n",
    "\n",
    "                loss = loss_fn(output, y)\n",
    "\n",
    "                metric_value = metric_fn(y.to('cpu'), y_pred.to('cpu'), average = 'macro')\n",
    "                valid_losses_epoch.append(loss.item())\n",
    "                valid_metric_epoch.append(metric_value)\n",
    "\n",
    "        valid_metric.append(np.mean(valid_metric_epoch))\n",
    "        valid_losses.append(np.mean(valid_losses_epoch))\n",
    "        valid_time += (time.time() - begin_time)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "    return train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwn8hIogbgxH"
   },
   "source": [
    "## Написание моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gXwH2kwbncZ"
   },
   "source": [
    "В качестве моделей мы будем использовать сверточные нейронные сети и различные ансамбли."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7GbaDHtpIWC"
   },
   "source": [
    "### Сверточные сети\n",
    "\n",
    "Для перебора архитектур не будем заводить отдельного класса, а напишем один раз шаблон и будем его менять прям в коде, потому что далее все равно будет выполняться сохранение моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uNHVGjg0ziL9"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_classes = len(labels)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 8, 7)\n",
    "        self.c_act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 32, 3)\n",
    "        self.c_act2 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.c_act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.c_act4 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3)\n",
    "        self.c_act5 = nn.ReLU() \n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3)\n",
    "        self.c_act6 = nn.ReLU()\n",
    "\n",
    "        self.flattener = nn.Flatten()\n",
    "\n",
    "        self.bn3 = nn.BatchNorm1d(9216)\n",
    "\n",
    "        self.linear1 = nn.Linear(9216, 4096)\n",
    "        self.l_act1 = nn.ReLU()\n",
    "\n",
    "        self.linear2 = nn.Linear(4096, 1024)\n",
    "        self.l_act2 = nn.ReLU()\n",
    "\n",
    "        self.linear3 = nn.Linear(1024, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.c_act1(self.conv1(x)))\n",
    "        \n",
    "        x = self.c_act2(self.conv2(x))\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.pool3(self.c_act3(self.conv3(x)))\n",
    "        \n",
    "        x = self.c_act4(self.conv4(x))\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.pool5(self.c_act5(self.conv5(x)))\n",
    "        \n",
    "        x = self.c_act6(self.conv6(x))\n",
    "\n",
    "        x = self.flattener(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.l_act1(self.linear1(x))\n",
    "        \n",
    "        x = self.l_act2(self.linear2(x))\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eNsTMHN5MA2i"
   },
   "outputs": [],
   "source": [
    "def train_valid_save(model, artifact_config,\n",
    "                     preprocess_config,\n",
    "                     train_config):\n",
    "    \n",
    "    art = wandb.Artifact(**artifact_config)\n",
    "    exp_name = artifact_config['name']\n",
    "    print('Making dataloaders...')\n",
    "    train_dl, valid_dl = make_loaders(**preprocess_config)\n",
    "    clear_output()\n",
    "    train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time = train_valid(\n",
    "        model = model,\n",
    "        train_dl = train_dl,\n",
    "        valid_dl = valid_dl,\n",
    "        **train_config\n",
    "    )\n",
    "\n",
    "    # Saving model\n",
    "    torch.save(model.state_dict(),\n",
    "                './models/models/' + exp_name + '.pth')\n",
    "\n",
    "    epochs = train_config['max_epochs']\n",
    "    for_table = list(zip(range(1, epochs + 1), \n",
    "                         train_losses,\n",
    "                         valid_losses,\n",
    "                         train_metric,\n",
    "                         valid_metric)) \n",
    "    \n",
    "    tabled_cfg = wandb.Table(\n",
    "        columns = ['Epoch', 'Train losses', 'Valid losses', 'Train score', 'Valid score'],\n",
    "        data = for_table\n",
    "    )\n",
    "\n",
    "    # Model state dict\n",
    "    art.add_file('./models/models/' + exp_name + '.pth',\n",
    "                 name = 'state_dict.pth')\n",
    "    \n",
    "    # Losses and metrics\n",
    "    art.add(tabled_cfg, 'Losses and scores table')\n",
    "\n",
    "    # Add result description\n",
    "    result_config = {'Train time': train_time,\n",
    "                     'Valid time': valid_time,\n",
    "                     'Device': device}\n",
    "\n",
    "    # Add configuration\n",
    "    common_config = {'Preprocess': preprocess_config,\n",
    "                     'Training': train_config,\n",
    "                     'Resulting': result_config}\n",
    "\n",
    "    art.metadata = common_config\n",
    "\n",
    "    x = next(model.modules())\n",
    "    with open('./models/desc/' + exp_name + '.txt', 'w') as f:\n",
    "        f.write(str(x))\n",
    "\n",
    "    art.add_file('./models/desc/' + exp_name + '.txt',\n",
    "                 name = 'desc.txt')\n",
    "\n",
    "    wandb.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh802qOeGvRi",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'augmentate': True,\n",
    "    'image_shape': (100, 100),\n",
    "    'ex_amount': 2000\n",
    "}\n",
    "\n",
    "preprocess_config = {\n",
    "    'ds_cls': AdvancedCustomDataset,\n",
    "    'ds_params': dataset_config,\n",
    "    'train_bs': 128,\n",
    "    'valid_bs': 256,\n",
    "    'train_size': 0.9\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'opt_cls': torch.optim.Adam,\n",
    "    'loss_fn': nn.CrossEntropyLoss(),\n",
    "    'metric_fn': f1_score,\n",
    "    'max_epochs': 10,\n",
    "    'opt_params': {\n",
    "        'lr': 5e-4\n",
    "    },\n",
    "    'scheduler_cls': torch.optim.lr_scheduler.StepLR,\n",
    "    'scheduler_params': {\n",
    "        'step_size': 4,\n",
    "        'gamma': 0.55\n",
    "    },\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "artifact_config = {\n",
    "    'name': 'CNN_v.1',\n",
    "    'type': 'model',\n",
    "    'description':\n",
    "    '''Using advanced dataset with 3k images per class;\n",
    "    Edited train function: now saving model with the highest metric on valid dataset\n",
    "    '''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "aI70rjr9QrjM",
    "outputId": "9ab0bafa-62d0-45de-aeeb-a6fc23b16022",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses on train: []\n",
      "Losses on valid: []\n",
      "Metric on train: []\n",
      "Metric on valid: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/2534 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|                                          | 1/2534 [00:00<41:06,  1.03it/s]\u001B[A\n",
      "  0%|                                          | 2/2534 [00:01<35:01,  1.20it/s]\u001B[A\n",
      "  0%|                                          | 3/2534 [00:02<32:37,  1.29it/s]\u001B[A\n",
      "  0%|                                          | 4/2534 [00:03<31:56,  1.32it/s]\u001B[A\n",
      "  0%|                                          | 5/2534 [00:03<31:55,  1.32it/s]\u001B[A\n",
      "  0%|                                          | 6/2534 [00:04<32:06,  1.31it/s]\u001B[A\n",
      "  0%|                                          | 7/2534 [00:05<30:57,  1.36it/s]\u001B[A\n",
      "  0%|▏                                         | 8/2534 [00:06<30:26,  1.38it/s]\u001B[A\n",
      "  0%|▏                                         | 9/2534 [00:06<29:59,  1.40it/s]\u001B[A\n",
      "  0%|▏                                        | 10/2534 [00:07<29:39,  1.42it/s]\u001B[A\n",
      "  0%|▏                                        | 11/2534 [00:08<29:49,  1.41it/s]\u001B[A\n",
      "  0%|▏                                        | 12/2534 [00:08<31:09,  1.35it/s]\u001B[A\n",
      "  1%|▏                                        | 13/2534 [00:09<31:41,  1.33it/s]\u001B[A\n",
      "  1%|▏                                        | 14/2534 [00:10<31:40,  1.33it/s]\u001B[A\n",
      "  1%|▏                                        | 15/2534 [00:11<31:37,  1.33it/s]\u001B[A\n",
      "  1%|▎                                        | 16/2534 [00:11<31:07,  1.35it/s]\u001B[A\n",
      "  1%|▎                                        | 17/2534 [00:12<31:01,  1.35it/s]\u001B[A\n",
      "  1%|▎                                        | 18/2534 [00:13<30:35,  1.37it/s]\u001B[A\n",
      "  1%|▎                                        | 19/2534 [00:14<30:58,  1.35it/s]\u001B[A\n",
      "  1%|▎                                        | 20/2534 [00:14<30:55,  1.36it/s]\u001B[A\n",
      "  1%|▎                                        | 21/2534 [00:15<31:18,  1.34it/s]\u001B[A\n",
      "  1%|▎                                        | 22/2534 [00:16<31:25,  1.33it/s]\u001B[A\n",
      "  1%|▎                                        | 23/2534 [00:17<31:22,  1.33it/s]\u001B[A\n",
      "  1%|▍                                        | 24/2534 [00:17<31:19,  1.34it/s]\u001B[A\n",
      "  1%|▍                                        | 25/2534 [00:18<31:08,  1.34it/s]\u001B[A\n",
      "  1%|▍                                        | 26/2534 [00:19<30:51,  1.35it/s]\u001B[A\n",
      "  1%|▍                                        | 27/2534 [00:20<31:16,  1.34it/s]\u001B[A\n",
      "  1%|▍                                        | 28/2534 [00:20<31:09,  1.34it/s]\u001B[A\n",
      "  1%|▍                                        | 29/2534 [00:21<30:45,  1.36it/s]\u001B[A\n",
      "  1%|▍                                        | 30/2534 [00:22<31:19,  1.33it/s]\u001B[A\n",
      "  1%|▌                                        | 31/2534 [00:23<31:22,  1.33it/s]\u001B[A\n",
      "  1%|▌                                        | 32/2534 [00:23<31:42,  1.32it/s]\u001B[A\n",
      "  1%|▌                                        | 33/2534 [00:24<32:06,  1.30it/s]\u001B[A\n",
      "  1%|▌                                        | 34/2534 [00:25<32:39,  1.28it/s]\u001B[A\n",
      "  1%|▌                                        | 35/2534 [00:26<32:56,  1.26it/s]\u001B[A\n",
      "  1%|▌                                        | 36/2534 [00:27<33:50,  1.23it/s]\u001B[A\n",
      "  1%|▌                                        | 37/2534 [00:28<34:00,  1.22it/s]\u001B[A\n",
      "  1%|▌                                        | 38/2534 [00:28<33:43,  1.23it/s]\u001B[A\n",
      "  2%|▋                                        | 39/2534 [00:29<33:00,  1.26it/s]\u001B[A\n",
      "  2%|▋                                        | 40/2534 [00:30<32:51,  1.26it/s]\u001B[A\n",
      "  2%|▋                                        | 41/2534 [00:31<32:35,  1.27it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  2%|▋                                        | 42/2534 [00:31<32:47,  1.27it/s]\u001B[A\n",
      "  2%|▋                                        | 43/2534 [00:32<32:47,  1.27it/s]\u001B[A\n",
      "  2%|▋                                        | 44/2534 [00:33<32:35,  1.27it/s]\u001B[A\n",
      "  2%|▋                                        | 45/2534 [00:34<32:05,  1.29it/s]\u001B[A\n",
      "  2%|▋                                        | 46/2534 [00:35<32:24,  1.28it/s]\u001B[A\n",
      "  2%|▊                                        | 47/2534 [00:35<31:53,  1.30it/s]\u001B[A\n",
      "  2%|▊                                        | 48/2534 [00:36<31:29,  1.32it/s]\u001B[A\n",
      "  2%|▊                                        | 49/2534 [00:37<31:44,  1.30it/s]\u001B[A\n",
      "  2%|▊                                        | 50/2534 [00:38<32:10,  1.29it/s]\u001B[A\n",
      "  2%|▊                                        | 51/2534 [00:38<32:37,  1.27it/s]\u001B[A\n",
      "  2%|▊                                        | 52/2534 [00:39<32:31,  1.27it/s]\u001B[A\n",
      "  2%|▊                                        | 53/2534 [00:40<32:27,  1.27it/s]\u001B[A\n",
      "  2%|▊                                        | 54/2534 [00:41<31:32,  1.31it/s]\u001B[A\n",
      "  2%|▉                                        | 55/2534 [00:41<31:36,  1.31it/s]\u001B[A\n",
      "  2%|▉                                        | 56/2534 [00:42<31:25,  1.31it/s]\u001B[A\n",
      "  2%|▉                                        | 57/2534 [00:43<31:49,  1.30it/s]\u001B[A\n",
      "  2%|▉                                        | 58/2534 [00:44<31:46,  1.30it/s]\u001B[A\n",
      "  2%|▉                                        | 59/2534 [00:45<31:46,  1.30it/s]\u001B[A\n",
      "  2%|▉                                        | 60/2534 [00:45<32:04,  1.29it/s]\u001B[A\n",
      "  2%|▉                                        | 61/2534 [00:46<32:04,  1.29it/s]\u001B[A\n",
      "  2%|█                                        | 62/2534 [00:47<31:57,  1.29it/s]\u001B[A\n",
      "  2%|█                                        | 63/2534 [00:48<32:04,  1.28it/s]\u001B[A\n",
      "  3%|█                                        | 64/2534 [00:48<32:10,  1.28it/s]\u001B[A\n",
      "  3%|█                                        | 65/2534 [00:49<32:13,  1.28it/s]\u001B[A\n",
      "  3%|█                                        | 66/2534 [00:50<32:10,  1.28it/s]\u001B[A\n",
      "  3%|█                                        | 67/2534 [00:51<31:45,  1.29it/s]\u001B[A\n",
      "  3%|█                                        | 68/2534 [00:52<31:25,  1.31it/s]\u001B[A\n",
      "  3%|█                                        | 69/2534 [00:52<32:03,  1.28it/s]\u001B[A\n",
      "  3%|█▏                                       | 70/2534 [00:53<32:58,  1.25it/s]\u001B[A\n",
      "  3%|█▏                                       | 71/2534 [00:54<33:42,  1.22it/s]\u001B[A\n",
      "  3%|█▏                                       | 72/2534 [00:55<34:32,  1.19it/s]\u001B[A\n",
      "  3%|█▏                                       | 73/2534 [00:56<34:21,  1.19it/s]\u001B[A\n",
      "  3%|█▏                                       | 74/2534 [00:57<34:09,  1.20it/s]\u001B[A\n",
      "  3%|█▏                                       | 75/2534 [00:58<34:53,  1.17it/s]\u001B[A\n",
      "  3%|█▏                                       | 76/2534 [00:58<34:19,  1.19it/s]\u001B[A\n",
      "  3%|█▏                                       | 77/2534 [00:59<34:34,  1.18it/s]\u001B[A\n",
      "  3%|█▎                                       | 78/2534 [01:00<34:58,  1.17it/s]\u001B[A\n",
      "  3%|█▎                                       | 79/2534 [01:01<35:13,  1.16it/s]\u001B[A\n",
      "  3%|█▎                                       | 80/2534 [01:02<34:27,  1.19it/s]\u001B[A\n",
      "  3%|█▎                                       | 81/2534 [01:03<34:21,  1.19it/s]\u001B[A\n",
      "  3%|█▎                                       | 82/2534 [01:03<34:29,  1.19it/s]\u001B[A\n",
      "  3%|█▎                                       | 83/2534 [01:04<34:22,  1.19it/s]\u001B[A\n",
      "  3%|█▎                                       | 84/2534 [01:05<34:55,  1.17it/s]\u001B[A\n",
      "  3%|█▍                                       | 85/2534 [01:06<35:21,  1.15it/s]\u001B[A\n",
      "  3%|█▍                                       | 86/2534 [01:07<34:43,  1.18it/s]\u001B[A\n",
      "  3%|█▍                                       | 87/2534 [01:08<35:00,  1.17it/s]\u001B[A\n",
      "  3%|█▍                                       | 88/2534 [01:09<35:18,  1.15it/s]\u001B[A\n",
      "  4%|█▍                                       | 89/2534 [01:09<35:37,  1.14it/s]\u001B[A\n",
      "  4%|█▍                                       | 90/2534 [01:10<35:19,  1.15it/s]\u001B[A\n",
      "  4%|█▍                                       | 91/2534 [01:11<35:09,  1.16it/s]\u001B[A\n",
      "  4%|█▍                                       | 92/2534 [01:12<34:49,  1.17it/s]\u001B[A\n",
      "  4%|█▌                                       | 93/2534 [01:13<34:36,  1.18it/s]\u001B[A\n",
      "  4%|█▌                                       | 94/2534 [01:14<34:37,  1.17it/s]\u001B[A\n",
      "  4%|█▌                                       | 95/2534 [01:15<34:40,  1.17it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                       | 96/2534 [01:15<34:58,  1.16it/s]\u001B[A\n",
      "  4%|█▌                                       | 97/2534 [01:16<34:41,  1.17it/s]\u001B[A\n",
      "  4%|█▌                                       | 98/2534 [01:17<34:31,  1.18it/s]\u001B[A\n",
      "  4%|█▌                                       | 99/2534 [01:18<34:34,  1.17it/s]\u001B[A\n",
      "  4%|█▌                                      | 100/2534 [01:19<34:38,  1.17it/s]\u001B[A\n",
      "  4%|█▌                                      | 101/2534 [01:20<33:33,  1.21it/s]\u001B[A\n",
      "  4%|█▌                                      | 102/2534 [01:21<34:35,  1.17it/s]\u001B[A\n",
      "  4%|█▋                                      | 103/2534 [01:21<35:03,  1.16it/s]\u001B[A\n",
      "  4%|█▋                                      | 104/2534 [01:22<36:05,  1.12it/s]\u001B[A\n",
      "  4%|█▋                                      | 105/2534 [01:23<36:57,  1.10it/s]\u001B[A\n",
      "  4%|█▋                                      | 106/2534 [01:24<37:29,  1.08it/s]\u001B[A\n",
      "  4%|█▋                                      | 107/2534 [01:25<36:42,  1.10it/s]\u001B[A\n",
      "  4%|█▋                                      | 108/2534 [01:26<36:37,  1.10it/s]\u001B[A\n",
      "  4%|█▋                                      | 109/2534 [01:27<37:21,  1.08it/s]\u001B[A\n",
      "  4%|█▋                                      | 110/2534 [01:28<38:36,  1.05it/s]\u001B[A\n",
      "  4%|█▊                                      | 111/2534 [01:29<38:22,  1.05it/s]\u001B[A\n",
      "  4%|█▊                                      | 112/2534 [01:30<38:35,  1.05it/s]\u001B[A\n",
      "  4%|█▊                                      | 113/2534 [01:31<37:47,  1.07it/s]\u001B[A\n",
      "  4%|█▊                                      | 114/2534 [01:32<37:18,  1.08it/s]\u001B[A\n",
      "  5%|█▊                                      | 115/2534 [01:33<36:51,  1.09it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  5%|█▊                                      | 116/2534 [01:34<37:45,  1.07it/s]\u001B[A\n",
      "  5%|█▊                                      | 117/2534 [01:35<38:44,  1.04it/s]\u001B[A\n",
      "  5%|█▊                                      | 118/2534 [01:36<38:15,  1.05it/s]\u001B[A\n",
      "  5%|█▉                                      | 119/2534 [01:36<37:28,  1.07it/s]\u001B[A\n",
      "  5%|█▉                                      | 120/2534 [01:37<38:15,  1.05it/s]\u001B[A\n",
      "  5%|█▉                                      | 121/2534 [01:39<40:03,  1.00it/s]\u001B[A\n",
      "  5%|█▉                                      | 122/2534 [01:40<40:44,  1.01s/it]\u001B[A\n",
      "  5%|█▉                                      | 123/2534 [01:41<39:14,  1.02it/s]\u001B[A\n",
      "  5%|█▉                                      | 124/2534 [01:41<38:20,  1.05it/s]\u001B[A\n",
      "  5%|█▉                                      | 125/2534 [01:42<37:29,  1.07it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  5%|█▉                                      | 126/2534 [01:43<36:52,  1.09it/s]\u001B[A\n",
      "  5%|██                                      | 127/2534 [01:44<36:09,  1.11it/s]\u001B[A\n",
      "  5%|██                                      | 128/2534 [01:45<36:21,  1.10it/s]\u001B[A\n",
      "  5%|██                                      | 129/2534 [01:46<36:27,  1.10it/s]\u001B[A\n",
      "  5%|██                                      | 130/2534 [01:47<36:04,  1.11it/s]\u001B[A\n",
      "  5%|██                                      | 131/2534 [01:48<35:54,  1.12it/s]\u001B[A\n",
      "  5%|██                                      | 132/2534 [01:49<35:24,  1.13it/s]\u001B[A\n",
      "  5%|██                                      | 133/2534 [01:49<35:30,  1.13it/s]\u001B[A\n",
      "  5%|██                                      | 134/2534 [01:50<35:15,  1.13it/s]\u001B[A\n",
      "  5%|██▏                                     | 135/2534 [01:51<36:04,  1.11it/s]\u001B[A\n",
      "  5%|██▏                                     | 136/2534 [01:52<36:11,  1.10it/s]\u001B[A\n",
      "  5%|██▏                                     | 137/2534 [01:53<36:23,  1.10it/s]\u001B[A\n",
      "  5%|██▏                                     | 138/2534 [01:54<36:44,  1.09it/s]\u001B[A\n",
      "  5%|██▏                                     | 139/2534 [01:55<36:25,  1.10it/s]\u001B[A\n",
      "  6%|██▏                                     | 140/2534 [01:56<36:25,  1.10it/s]\u001B[A\n",
      "  6%|██▏                                     | 141/2534 [01:57<36:12,  1.10it/s]\u001B[A\n",
      "  6%|██▏                                     | 142/2534 [01:58<35:44,  1.12it/s]\u001B[A\n",
      "  6%|██▎                                     | 143/2534 [01:58<35:38,  1.12it/s]\u001B[A\n",
      "  6%|██▎                                     | 144/2534 [01:59<35:58,  1.11it/s]\u001B[A\n",
      "  6%|██▎                                     | 145/2534 [02:00<34:48,  1.14it/s]\u001B[A\n",
      "  6%|██▎                                     | 146/2534 [02:01<34:56,  1.14it/s]\u001B[A\n",
      "  6%|██▎                                     | 147/2534 [02:02<35:27,  1.12it/s]\u001B[A\n",
      "  6%|██▎                                     | 148/2534 [02:03<35:18,  1.13it/s]\u001B[A\n",
      "  6%|██▎                                     | 149/2534 [02:04<34:51,  1.14it/s]\u001B[A\n",
      "  6%|██▎                                     | 150/2534 [02:05<35:26,  1.12it/s]\u001B[A\n",
      "  6%|██▍                                     | 151/2534 [02:06<35:49,  1.11it/s]\u001B[A\n",
      "  6%|██▍                                     | 152/2534 [02:06<35:27,  1.12it/s]\u001B[A\n",
      "  6%|██▍                                     | 153/2534 [02:07<35:13,  1.13it/s]\u001B[A\n",
      "  6%|██▍                                     | 154/2534 [02:08<35:10,  1.13it/s]\u001B[A\n",
      "  6%|██▍                                     | 155/2534 [02:09<35:01,  1.13it/s]\u001B[A\n",
      "  6%|██▍                                     | 156/2534 [02:10<36:39,  1.08it/s]\u001B[A\n",
      "  6%|██▍                                     | 157/2534 [02:11<36:46,  1.08it/s]\u001B[A\n",
      "  6%|██▍                                     | 158/2534 [02:12<36:50,  1.08it/s]\u001B[A\n",
      "  6%|██▌                                     | 159/2534 [02:13<35:41,  1.11it/s]\u001B[A\n",
      "  6%|██▌                                     | 160/2534 [02:14<35:12,  1.12it/s]\u001B[A\n",
      "  6%|██▌                                     | 161/2534 [02:15<35:07,  1.13it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  6%|██▌                                     | 162/2534 [02:15<35:30,  1.11it/s]\u001B[A\n",
      "  6%|██▌                                     | 163/2534 [02:16<35:52,  1.10it/s]\u001B[A\n",
      "  6%|██▌                                     | 164/2534 [02:17<36:22,  1.09it/s]\u001B[A\n",
      "  7%|██▌                                     | 165/2534 [02:18<36:02,  1.10it/s]\u001B[A\n",
      "  7%|██▌                                     | 166/2534 [02:19<35:36,  1.11it/s]\u001B[A\n",
      "  7%|██▋                                     | 167/2534 [02:20<35:40,  1.11it/s]\u001B[A\n",
      "  7%|██▋                                     | 168/2534 [02:21<35:16,  1.12it/s]\u001B[A\n",
      "  7%|██▋                                     | 169/2534 [02:22<34:52,  1.13it/s]\u001B[A\n",
      "  7%|██▋                                     | 170/2534 [02:23<34:58,  1.13it/s]\u001B[A\n",
      "  7%|██▋                                     | 171/2534 [02:24<35:30,  1.11it/s]\u001B[A\n",
      "  7%|██▋                                     | 172/2534 [02:24<35:01,  1.12it/s]\u001B[A\n",
      "  7%|██▋                                     | 173/2534 [02:25<35:10,  1.12it/s]\u001B[A\n",
      "  7%|██▋                                     | 174/2534 [02:26<34:48,  1.13it/s]\u001B[A\n",
      "  7%|██▊                                     | 175/2534 [02:27<35:22,  1.11it/s]\u001B[A\n",
      "  7%|██▊                                     | 176/2534 [02:28<35:44,  1.10it/s]\u001B[A\n",
      "  7%|██▊                                     | 177/2534 [02:29<35:05,  1.12it/s]\u001B[A\n",
      "  7%|██▊                                     | 178/2534 [02:30<35:12,  1.12it/s]\u001B[A\n",
      "  7%|██▊                                     | 179/2534 [02:31<35:55,  1.09it/s]\u001B[A\n",
      "  7%|██▊                                     | 180/2534 [02:32<35:15,  1.11it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  7%|██▊                                     | 181/2534 [02:33<35:23,  1.11it/s]\u001B[A\n",
      "  7%|██▊                                     | 182/2534 [02:33<35:00,  1.12it/s]\u001B[A\n",
      "  7%|██▉                                     | 183/2534 [02:34<35:13,  1.11it/s]\u001B[A\n",
      "  7%|██▉                                     | 184/2534 [02:35<35:55,  1.09it/s]\u001B[A\n",
      "  7%|██▉                                     | 185/2534 [02:36<35:51,  1.09it/s]\u001B[A\n",
      "  7%|██▉                                     | 186/2534 [02:37<35:45,  1.09it/s]\u001B[A\n",
      "  7%|██▉                                     | 187/2534 [02:38<35:59,  1.09it/s]\u001B[A\n",
      "  7%|██▉                                     | 188/2534 [02:39<35:59,  1.09it/s]\u001B[A\n",
      "  7%|██▉                                     | 189/2534 [02:40<35:55,  1.09it/s]\u001B[A\n",
      "  7%|██▉                                     | 190/2534 [02:41<36:25,  1.07it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███                                     | 191/2534 [02:42<36:35,  1.07it/s]\u001B[A\n",
      "  8%|███                                     | 192/2534 [02:43<36:29,  1.07it/s]\u001B[A\n",
      "  8%|███                                     | 193/2534 [02:44<35:57,  1.09it/s]\u001B[A\n",
      "  8%|███                                     | 194/2534 [02:45<35:50,  1.09it/s]\u001B[A\n",
      "  8%|███                                     | 195/2534 [02:45<35:31,  1.10it/s]\u001B[A\n",
      "  8%|███                                     | 196/2534 [02:46<35:21,  1.10it/s]\u001B[A\n",
      "  8%|███                                     | 197/2534 [02:47<35:08,  1.11it/s]\u001B[A\n",
      "  8%|███▏                                    | 198/2534 [02:48<35:09,  1.11it/s]\u001B[A\n",
      "  8%|███▏                                    | 199/2534 [02:49<34:51,  1.12it/s]\u001B[A\n",
      "  8%|███▏                                    | 200/2534 [02:50<34:30,  1.13it/s]\u001B[A\n",
      "  8%|███▏                                    | 201/2534 [02:51<34:31,  1.13it/s]\u001B[A\n",
      "  8%|███▏                                    | 202/2534 [02:52<34:44,  1.12it/s]\u001B[A\n",
      "  8%|███▏                                    | 203/2534 [02:53<34:34,  1.12it/s]\u001B[A\n",
      "  8%|███▏                                    | 204/2534 [02:53<34:36,  1.12it/s]\u001B[A\n",
      "  8%|███▏                                    | 205/2534 [02:54<34:46,  1.12it/s]\u001B[A\n",
      "  8%|███▎                                    | 206/2534 [02:55<35:27,  1.09it/s]\u001B[A\n",
      "  8%|███▎                                    | 207/2534 [02:56<35:32,  1.09it/s]\u001B[A\n",
      "  8%|███▎                                    | 208/2534 [02:57<35:59,  1.08it/s]\u001B[A\n",
      "  8%|███▎                                    | 209/2534 [02:58<36:11,  1.07it/s]\u001B[A\n",
      "  8%|███▎                                    | 210/2534 [02:59<36:10,  1.07it/s]\u001B[A\n",
      "  8%|███▎                                    | 211/2534 [03:00<36:08,  1.07it/s]\u001B[A\n",
      "  8%|███▎                                    | 212/2534 [03:01<36:15,  1.07it/s]\u001B[A\n",
      "  8%|███▎                                    | 213/2534 [03:02<36:15,  1.07it/s]\u001B[A\n",
      "  8%|███▍                                    | 214/2534 [03:03<35:52,  1.08it/s]\u001B[A\n",
      "  8%|███▍                                    | 215/2534 [03:04<36:10,  1.07it/s]\u001B[A\n",
      "  9%|███▍                                    | 216/2534 [03:05<36:26,  1.06it/s]\u001B[A\n",
      "  9%|███▍                                    | 217/2534 [03:06<36:34,  1.06it/s]\u001B[A\n",
      "  9%|███▍                                    | 218/2534 [03:07<36:21,  1.06it/s]\u001B[A\n",
      "  9%|███▍                                    | 219/2534 [03:08<35:58,  1.07it/s]\u001B[A\n",
      "  9%|███▍                                    | 220/2534 [03:08<35:19,  1.09it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  9%|███▍                                    | 221/2534 [03:09<35:42,  1.08it/s]\u001B[A\n",
      "  9%|███▌                                    | 222/2534 [03:10<35:30,  1.09it/s]\u001B[A\n",
      "  9%|███▌                                    | 223/2534 [03:11<35:10,  1.10it/s]\u001B[A\n",
      "  9%|███▌                                    | 224/2534 [03:12<34:53,  1.10it/s]\u001B[A\n",
      "  9%|███▌                                    | 225/2534 [03:13<34:32,  1.11it/s]\u001B[A\n",
      "  9%|███▌                                    | 226/2534 [03:14<34:14,  1.12it/s]\u001B[A\n",
      "  9%|███▌                                    | 227/2534 [03:15<34:03,  1.13it/s]\u001B[A\n",
      "  9%|███▌                                    | 228/2534 [03:16<34:16,  1.12it/s]\u001B[A\n",
      "  9%|███▌                                    | 229/2534 [03:16<33:55,  1.13it/s]\u001B[A\n",
      "  9%|███▋                                    | 230/2534 [03:17<34:12,  1.12it/s]\u001B[A\n",
      "  9%|███▋                                    | 231/2534 [03:18<34:16,  1.12it/s]\u001B[A\n",
      "  9%|███▋                                    | 232/2534 [03:19<34:10,  1.12it/s]\u001B[A\n",
      "  9%|███▋                                    | 233/2534 [03:20<34:25,  1.11it/s]\u001B[A\n",
      "  9%|███▋                                    | 234/2534 [03:21<34:01,  1.13it/s]\u001B[Alibpng warning: iCCP: known incorrect sRGB profile\n",
      "\n",
      "  9%|███▋                                    | 235/2534 [03:22<34:10,  1.12it/s]\u001B[A\n",
      "  9%|███▋                                    | 236/2534 [03:23<34:52,  1.10it/s]\u001B[A\n",
      "  9%|███▋                                    | 237/2534 [03:24<35:08,  1.09it/s]\u001B[A\n",
      "  9%|███▊                                    | 238/2534 [03:25<35:31,  1.08it/s]\u001B[A\n",
      "  9%|███▊                                    | 239/2534 [03:26<35:53,  1.07it/s]\u001B[A\n",
      "  9%|███▊                                    | 240/2534 [03:27<36:04,  1.06it/s]\u001B[A\n",
      " 10%|███▊                                    | 241/2534 [03:28<36:05,  1.06it/s]\u001B[A\n",
      " 10%|███▊                                    | 242/2534 [03:28<36:05,  1.06it/s]\u001B[A\n",
      " 10%|███▊                                    | 243/2534 [03:29<35:39,  1.07it/s]\u001B[A\n",
      " 10%|███▊                                    | 244/2534 [03:30<36:10,  1.06it/s]\u001B[A\n",
      " 10%|███▊                                    | 245/2534 [03:31<35:55,  1.06it/s]\u001B[A\n",
      " 10%|███▉                                    | 246/2534 [03:32<35:35,  1.07it/s]\u001B[A\n",
      " 10%|███▉                                    | 247/2534 [03:33<35:40,  1.07it/s]\u001B[A\n",
      " 10%|███▉                                    | 248/2534 [03:34<36:06,  1.06it/s]\u001B[A\n",
      " 10%|███▉                                    | 249/2534 [03:35<35:42,  1.07it/s]\u001B[A\n",
      " 10%|███▉                                    | 250/2534 [03:36<35:32,  1.07it/s]\u001B[A\n",
      " 10%|███▉                                    | 251/2534 [03:37<35:17,  1.08it/s]\u001B[A\n",
      " 10%|███▉                                    | 252/2534 [03:38<34:33,  1.10it/s]\u001B[A\n",
      " 10%|███▉                                    | 253/2534 [03:39<34:10,  1.11it/s]\u001B[A\n",
      " 10%|████                                    | 254/2534 [03:39<33:48,  1.12it/s]\u001B[A\n",
      " 10%|████                                    | 255/2534 [03:40<33:52,  1.12it/s]\u001B[A\n",
      " 10%|████                                    | 256/2534 [03:41<34:29,  1.10it/s]\u001B[A\n",
      " 10%|████                                    | 257/2534 [03:42<34:41,  1.09it/s]\u001B[A\n",
      " 10%|████                                    | 258/2534 [03:43<35:00,  1.08it/s]\u001B[A\n",
      " 10%|████                                    | 259/2534 [03:44<35:21,  1.07it/s]\u001B[A\n",
      " 10%|████                                    | 260/2534 [03:45<35:05,  1.08it/s]\u001B[A\n",
      " 10%|████                                    | 261/2534 [03:46<34:39,  1.09it/s]\u001B[A\n",
      " 10%|████▏                                   | 262/2534 [03:47<34:26,  1.10it/s]\u001B[A\n",
      " 10%|████▏                                   | 263/2534 [03:48<34:04,  1.11it/s]\u001B[A\n",
      " 10%|████▏                                   | 264/2534 [03:49<33:53,  1.12it/s]\u001B[A\n",
      " 10%|████▏                                   | 265/2534 [03:50<34:01,  1.11it/s]\u001B[A\n",
      " 10%|████▏                                   | 266/2534 [03:50<34:02,  1.11it/s]\u001B[A\n",
      " 11%|████▏                                   | 267/2534 [03:51<34:20,  1.10it/s]\u001B[A\n",
      " 11%|████▏                                   | 268/2534 [03:52<35:19,  1.07it/s]\u001B[A\n",
      " 11%|████▏                                   | 269/2534 [03:53<35:26,  1.07it/s]\u001B[A\n",
      " 11%|████▎                                   | 270/2534 [03:54<35:28,  1.06it/s]\u001B[A\n",
      " 11%|████▎                                   | 271/2534 [03:55<35:49,  1.05it/s]\u001B[A\n",
      " 11%|████▎                                   | 272/2534 [03:56<35:46,  1.05it/s]\u001B[A\n",
      " 11%|████▎                                   | 273/2534 [03:57<36:27,  1.03it/s]\u001B[A\n",
      " 11%|████▎                                   | 274/2534 [03:58<37:29,  1.00it/s]\u001B[A\n",
      " 11%|████▎                                   | 275/2534 [03:59<38:42,  1.03s/it]\u001B[A\n",
      " 11%|████▎                                   | 276/2534 [04:00<39:28,  1.05s/it]\u001B[A\n",
      " 11%|████▎                                   | 277/2534 [04:01<39:22,  1.05s/it]\u001B[A\n",
      " 11%|████▍                                   | 278/2534 [04:03<39:24,  1.05s/it]\u001B[A\n",
      " 11%|████▍                                   | 279/2534 [04:03<38:00,  1.01s/it]\u001B[A\n",
      " 11%|████▍                                   | 280/2534 [04:04<37:47,  1.01s/it]\u001B[A\n",
      " 11%|████▍                                   | 281/2534 [04:05<37:53,  1.01s/it]\u001B[A\n",
      " 11%|████▍                                   | 282/2534 [04:06<37:33,  1.00s/it]\u001B[A\n",
      " 11%|████▍                                   | 283/2534 [04:07<37:14,  1.01it/s]\u001B[A\n",
      " 11%|████▍                                   | 284/2534 [04:08<36:35,  1.02it/s]\u001B[A\n",
      " 11%|████▍                                   | 285/2534 [04:09<36:24,  1.03it/s]\u001B[A\n",
      " 11%|████▌                                   | 286/2534 [04:10<35:26,  1.06it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▌                                   | 287/2534 [04:11<35:27,  1.06it/s]\u001B[A\n",
      " 11%|████▌                                   | 288/2534 [04:12<35:11,  1.06it/s]\u001B[A\n",
      " 11%|████▌                                   | 289/2534 [04:13<34:22,  1.09it/s]\u001B[A\n",
      " 11%|████▌                                   | 290/2534 [04:14<33:57,  1.10it/s]\u001B[A\n",
      " 11%|████▌                                   | 291/2534 [04:15<33:35,  1.11it/s]\u001B[A\n",
      " 12%|████▌                                   | 292/2534 [04:16<34:16,  1.09it/s]\u001B[A\n",
      " 12%|████▋                                   | 293/2534 [04:17<33:57,  1.10it/s]\u001B[A\n",
      " 12%|████▋                                   | 294/2534 [04:17<33:21,  1.12it/s]\u001B[A\n",
      " 12%|████▋                                   | 295/2534 [04:18<33:50,  1.10it/s]\u001B[A\n",
      " 12%|████▋                                   | 296/2534 [04:19<34:31,  1.08it/s]\u001B[A\n",
      " 12%|████▋                                   | 297/2534 [04:20<35:25,  1.05it/s]\u001B[A\n",
      " 12%|████▋                                   | 298/2534 [04:21<36:24,  1.02it/s]\u001B[A\n",
      " 12%|████▋                                   | 299/2534 [04:22<36:37,  1.02it/s]\u001B[A\n",
      " 12%|████▋                                   | 300/2534 [04:23<35:31,  1.05it/s]\u001B[A\n",
      " 12%|████▊                                   | 301/2534 [04:24<34:49,  1.07it/s]\u001B[A\n",
      " 12%|████▊                                   | 302/2534 [04:25<34:30,  1.08it/s]\u001B[A\n",
      " 12%|████▊                                   | 303/2534 [04:26<33:59,  1.09it/s]\u001B[A\n",
      " 12%|████▊                                   | 304/2534 [04:27<33:45,  1.10it/s]\u001B[A\n",
      " 12%|████▊                                   | 305/2534 [04:28<33:41,  1.10it/s]\u001B[A\n",
      " 12%|████▊                                   | 306/2534 [04:29<33:32,  1.11it/s]\u001B[A\n",
      " 12%|████▊                                   | 307/2534 [04:29<32:59,  1.12it/s]\u001B[A\n",
      " 12%|████▊                                   | 308/2534 [04:31<32:44,  1.13it/s]\u001B[A\n",
      "Epoch:   0%|                                             | 0/10 [04:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_valid_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                 \u001B[49m\u001B[43martifact_config\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43martifact_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mpreprocess_config\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpreprocess_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtrain_config\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[26], line 10\u001B[0m, in \u001B[0;36mtrain_valid_save\u001B[0;34m(model, artifact_config, preprocess_config, train_config)\u001B[0m\n\u001B[1;32m      8\u001B[0m train_dl, valid_dl \u001B[38;5;241m=\u001B[39m make_loaders(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_config)\n\u001B[1;32m      9\u001B[0m clear_output()\n\u001B[0;32m---> 10\u001B[0m train_losses, valid_losses, train_metric, valid_metric, train_time, valid_time \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_valid\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalid_dl\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mvalid_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtrain_config\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Saving model\u001B[39;00m\n\u001B[1;32m     18\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[1;32m     19\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/models/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m exp_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[24], line 68\u001B[0m, in \u001B[0;36mtrain_valid\u001B[0;34m(model, train_dl, valid_dl, opt_cls, opt_params, loss_fn, metric_fn, max_epochs, device, scheduler_cls, scheduler_params)\u001B[0m\n\u001B[1;32m     66\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     67\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 68\u001B[0m metric_value \u001B[38;5;241m=\u001B[39m metric_fn(\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m, y_pred\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m), average \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmacro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     69\u001B[0m train_metric_epoch\u001B[38;5;241m.\u001B[39mappend(metric_value)\n\u001B[1;32m     70\u001B[0m train_losses_epoch\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_valid_save(model = model,\n",
    "                 artifact_config = artifact_config,\n",
    "                 preprocess_config = preprocess_config,\n",
    "                 train_config = train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6cBA_X_5yh0",
    "outputId": "bc5bc5e3-740f-441d-bb85-295209efdebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 22 22:50:04 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   82C    P0    16W /  50W |   2473MiB /  4096MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1788      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A     53062      C   /usr/bin/python3                 2466MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "mpHbAJysMGPv",
    "outputId": "2e4e3988-27ae-4077-cf47-c5dfdd15a827"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-gorge-22</strong> at: <a href='https://wandb.ai/ml_landmarks/ml_landmarks/runs/ik14x1u5' target=\"_blank\">https://wandb.ai/ml_landmarks/ml_landmarks/runs/ik14x1u5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230422_223740-ik14x1u5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuP8McSV9fvi"
   },
   "source": [
    "Я пытался поменять способ рейшейпинга, но время обучения увеличилось в 2.5 раза.\n",
    "Возможно, что это из-за weight decay.\n",
    "\n",
    "Добавить батчнорм\n",
    "\n",
    "ArcFace?\n",
    "\n",
    "Лоссы вообще посмотреть\n",
    "\n",
    "Гугл датасет сохранил на кагл\n",
    "\n",
    "Файнтьюн?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD8IxRDrZNqC"
   },
   "source": [
    "# TODO:\n",
    "<font color='red'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFtWTJNykwdP"
   },
   "source": [
    "## Затухание градиентов\n",
    "\n",
    "Кажется, что градиенты затухают даже для CNN v.1. Что можно сделать?\n",
    "\n",
    "*   Weight decay\n",
    "*   Регуляризация\n",
    "*   Skip-connections\n",
    "*   Поиграть с lr\n",
    "*   Клипать градиенты\n",
    "*   Шедулеры? Добавил\n",
    "*   Менять датасет мб уже пора(\n",
    "*   Можно попробовать добавить второй лосс после сверточного блока (как, по моему, в Inception)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GILf8neHv0gE"
   },
   "source": [
    "## Ускорение инференса и обучение\n",
    "\n",
    "*    Добавить батчнорм\n",
    "*    Прунинг (надо чтоб она хоть какое то качество показала)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7XNEgDVwbTA"
   },
   "source": [
    "## Интерактивное создание сетей\n",
    "\n",
    "Тут я пытался сделать небольшую утилку, которая всякими слайдерами и дропдаунами могла бы генерить слои нейронной сети. Просто по приколу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OlIrXxx3uIe"
   },
   "outputs": [],
   "source": [
    "def selector(**kwargs):\n",
    "\n",
    "    def conv(**kwargs):\n",
    "        return nn.Conv2d(**kwargs)\n",
    "    \n",
    "    def linear(**kwargs):\n",
    "        return nn.Linear(**kwargs)\n",
    "\n",
    "    def activation(**kwargs):\n",
    "        cls = kwargs['cls']\n",
    "        if cls == 'ReLU':\n",
    "            return nn.ReLU()\n",
    "        if cls == 'Tanh':\n",
    "            return nn.Tanh()\n",
    "        if cls == 'Sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "    \n",
    "    def dropout(**kwargs):\n",
    "        return nn.Dropout1d(**kwargs)\n",
    "\n",
    "    def pooling(**kwargs):\n",
    "        pool_type = kwargs['pool_type']\n",
    "        del kwargs['pool_type']\n",
    "        if pool_type == 'max':\n",
    "            return nn.MaxPool2d(**kwargs)\n",
    "    \n",
    "    layer = kwargs['layer_type']\n",
    "    if layer == 'Conv2d':\n",
    "        w = widgets.interact_manual(conv,\n",
    "                                    in_channels = widgets.IntText(value = 1),\n",
    "                                   out_channels = widgets.IntText(value = 1),\n",
    "                                   kernel_size = widgets.IntText(value = 1),\n",
    "                                   padding = widgets.IntText(value = 0),\n",
    "                                   stride = widgets.IntText(value = 1))\n",
    "    \n",
    "    elif layer == 'Linear':\n",
    "        w = widgets.interactive(linear, {'manual': True, 'auto_display': True},\n",
    "                                   in_features = widgets.IntText(value = 1),\n",
    "                                   out_features = widgets.IntText(value = 1))\n",
    "        print(f'Linear {w}')\n",
    "\n",
    "    elif layer == 'Activation':\n",
    "        w = widgets.interactive(activation, \n",
    "                                {'manual': True, 'auto_display': True},\n",
    "                                cls = ['ReLU', 'Tanh', 'Sigmoid'])\n",
    "        print(f'ww - {w}')\n",
    "\n",
    "    elif layer == 'Dropout':\n",
    "        w = widgets.interact_manual(dropout,\n",
    "                                   p = widgets.FloatText(value = 0.5,\n",
    "                                                         min = 0,\n",
    "                                                         max = 1))\n",
    "    \n",
    "    elif layer == 'Pooling':\n",
    "        w =  widgets.interact_manual(pooling,\n",
    "                                 pool_type = ['max', 'avg', 'min'],\n",
    "                                 kernel_size = widgets.IntText(value = 1),\n",
    "                                 padding = widgets.IntText(value = 0),\n",
    "                                 stride = widgets.IntText(value = 1))\n",
    "        \n",
    "    display(w)\n",
    "    print(f'w = {w}')\n",
    "    return w\n",
    "\n",
    "\n",
    "result = widgets.interactive(selector, layer_type = ['Conv2d',\n",
    "                                                  'Pooling',\n",
    "                                                  'Linear',\n",
    "                                                  'Activation',\n",
    "                                                  'Dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "e2a8ea8d0e784f809f0e1d3418abbd10"
     ]
    },
    "id": "j7inGNDvBlb7",
    "outputId": "27f3d26e-6b44-4a0a-ff5b-ea0698582d9d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a8ea8d0e784f809f0e1d3418abbd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='layer_type', options=('Conv2d', 'Pooling', 'Linear', 'Activation',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = (Dropdown(description='layer_type', options=('Conv2d', 'Pooling', 'Linear', 'Activation', 'Dropout'), value='Conv2d'), Output())\n"
     ]
    }
   ],
   "source": [
    "display(result)\n",
    "z = result.children\n",
    "print(f'z = {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGoydRmTeasL"
   },
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    _number = 1\n",
    "    def __init__(self, conv_params, conv_cls = nn.Conv2d,\n",
    "                 pooling_cls = None, pooling_params = None,\n",
    "                 activation_cls = None, activation_params = None):\n",
    "        '''\n",
    "        Create conv layer: conv2d->pooling->activation\n",
    "        *_params - dict with layer params\n",
    "        *_cls - class of layer\n",
    "        '''\n",
    "        number = ConvLayer._number\n",
    "        super().__init__()\n",
    "        conv_layer = conv_cls(**conv_params)\n",
    "        self.conv_layer = nn.Sequential()\n",
    "        self.conv_layer.add_module(f'Conv_{number}', conv_layer)\n",
    "\n",
    "        if pooling_cls:\n",
    "            pooling_layer = pooling_cls(**pooling_params)\n",
    "            self.conv_layer.add_module(f'Pooling_{number}', pooling_layer)\n",
    "\n",
    "        if activation_cls:\n",
    "            activation = activation_cls(**activation_params)\n",
    "            self.conv_layer.add_module(f'Activation_{number}', activation)\n",
    "\n",
    "        ConvLayer._number += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8pcv-2Pp5Eo"
   },
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    _number = 1\n",
    "    def __init__(self, linear_params, linear_cls = nn.Linear,\n",
    "                 dropout_cls = None, dropout_params = None,\n",
    "                 activation_cls = None, activation_params = None):\n",
    "        '''\n",
    "        Create FC-layer: dropout->linear->activation\n",
    "        If current layer is last, actvation can be replaced on\n",
    "        something like Softmax, etc.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.fc_layer = nn.Sequential()\n",
    "        number = FCLayer._number\n",
    "        if dropout_cls:\n",
    "            dropout_layer = dropout_cls(**dropout_params)\n",
    "            self.fc_layer.add_module(f'Dropout_{number}', dropout_layer)\n",
    "        \n",
    "        linear = linear_cls(**linear_params)\n",
    "        self.fc_layer.add_module(f'Linear_{number}', linear)\n",
    "        \n",
    "        if activation_cls:\n",
    "            if activation_params:\n",
    "                activation = activation_cls(**activation_params)\n",
    "            else:\n",
    "                activation = activation_cls()\n",
    "            self.fc_layer.add_module(f'Activation_{number}', activation)\n",
    "\n",
    "        FCLayer._number += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KU5V8oC7jpDm"
   },
   "outputs": [],
   "source": [
    "class MiddleLayer(nn.Module):\n",
    "    _number = 1\n",
    "    def __init__(self, params = None):\n",
    "        '''\n",
    "        Create layers with middle layers\n",
    "        params - tuple (layer_cls, dict(layer_params))\n",
    "        '''\n",
    "        super().__init__()\n",
    "        number = MiddleLayer._number\n",
    "        if isinstance(params, (tuple, list)):\n",
    "            self.mid_layer = nn.Sequential()\n",
    "            for elem in params:\n",
    "                cls, cls_params = elem\n",
    "                if cls_params:\n",
    "                    layer = cls(**cls_params)\n",
    "                else:\n",
    "                    layer = cls()\n",
    "\n",
    "                self.mid_layer.add_module(f'Mid_{cls.__name__}_{number}', layer)\n",
    "        else:\n",
    "            self.mid_layer = None\n",
    "\n",
    "        MiddleLayer._number += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.mid_layer:\n",
    "            return self.mid_layer(x)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wWTg55Bbu3w"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, batch_norm = False,\n",
    "                 conv_layers = None,\n",
    "                 middle_layers = None,\n",
    "                 fc_layers = None):\n",
    "        '''\n",
    "        Generate architecture of neural net.\n",
    "        batch_norm - bool. If True, add batchnorm to input\n",
    "        conv_layers - list with conv layers\n",
    "        middle_layers - list with middle layers (mb bottleneck/flatten)\n",
    "        fc_layers - list with fc layers\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm2d(3) # Для RGB картинок.\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.middle_layers = nn.Sequential(*middle_layers)\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        \n",
    "        if self.conv_layers:\n",
    "            x = self.conv_layers(x)\n",
    "\n",
    "        if self.middle_layers:\n",
    "            x = self.middle_layers(x)\n",
    "\n",
    "        if self.fc_layers:\n",
    "            x = self.fc_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def readable_config(self):\n",
    "        # Saving readable config\n",
    "        x = cnn.children()\n",
    "        ans = ''\n",
    "        for y in x:\n",
    "            ans += str(y) + '\\n\\n'\n",
    "        return ans"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SnSm8eSra48Z",
    "8dyv0_7gbQyx",
    "Q547p3nH9Llz",
    "lhrc_O1H6wGT",
    "62u_hQUT7Dvb",
    "8tPpz2ClN7wH",
    "uD8IxRDrZNqC",
    "iFtWTJNykwdP",
    "GILf8neHv0gE",
    "Q7XNEgDVwbTA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
